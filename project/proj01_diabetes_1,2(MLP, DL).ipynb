{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQLLz9mKwrJW5zBUxdm/ZW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# MLP Regression using Diabetes dataset from sklearn with Keras\n","\n","- 케라스를 이용하여 당뇨병 데이터를 사용하여 MLP regression 프로그램 만들기"],"metadata":{"id":"Zywnn0a4V6NO"}},{"cell_type":"markdown","source":["## proj01_diabetes_1_MLP"],"metadata":{"id":"d9287ku9WasR"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vQEnWZAEV09D","executionInfo":{"status":"ok","timestamp":1681888315470,"user_tz":-540,"elapsed":1126,"user":{"displayName":"김민제","userId":"08475270451301796366"}},"outputId":"84efbc65-4278-41ca-b5b9-58b02056c805"},"outputs":[{"output_type":"stream","name":"stdout","text":["R^2 score: 0.4536408809258685\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]}],"source":["# -*- coding: utf-8 -*-\n","\n","from sklearn.datasets import load_diabetes\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import MLPRegressor\n","\n","# Load the diabetes dataset\n","diabetes = load_diabetes()\n","\n","# Preprocess the data\n","X = StandardScaler().fit_transform(diabetes.data)\n","y = diabetes.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create an MLP model\n","mlp = MLPRegressor(hidden_layer_sizes=(16, 8), max_iter=1000)\n","\n","# Train the model\n","mlp.fit(X_train, y_train)\n","\n","# Evaluate the model\n","score = mlp.score(X_test, y_test)\n","print(\"R^2 score:\", score)\n","\n","# 은닉층 사이즈 바꿔주면 성능 더 좋아짐\n","# R^2 score: 0.46150686623228765, hidden_layer_sizes=(16, 8)\n","# R^2 score: 0.4380976292446448, hidden_layer_sizes=(16, 8)\n","# R^2 score: 0.47201385144961695, hidden_layer_sizes=(100, 50)\n","# R^2 score: 0.48616233328458436, hidden_layer_sizes=(100, 50)"]},{"cell_type":"markdown","source":["## proj01_diabetes_2_DL"],"metadata":{"id":"7IOFzSi_XH2O"}},{"cell_type":"code","source":["from sklearn.datasets import load_diabetes\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import r2_score\n","from keras.models import Sequential\n","from keras.layers import Dense\n","\n","# Load data\n","diabetes = load_diabetes()\n","# Preprocess the data\n","X = StandardScaler().fit_transform(diabetes.data)\n","y = diabetes.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n","# Build model\n","model = Sequential()\n","model.add(Dense(16, activation='relu', input_dim=X.shape[1]))\n","model.add(Dense(8, activation='relu'))\n","model.add(Dense(1, activation='linear'))\n","\n","# Compile model\n","model.compile(loss='mse', optimizer='adam')\n","\n","# Train model\n","model.fit(X_train, y_train, epochs=200, batch_size=10, validation_split=0.2) # 80%의 데이터를 학습시키고 동시에 0.2 즉 20%의 데이터를 가지고 평가함\n","\n","# Evaluate model\n","y_pred = model.predict(X_test)  #, y_test)\n","score = r2_score(y_test, y_pred)\n","print(\"R^2 score:\", score)\n","# R^2 score: 0.4121845510590546\n","# R^2 score: 0.4787058293145606\n","# R^2 score: 0.5015287710964408"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"avIlMk2tXKpN","executionInfo":{"status":"ok","timestamp":1681888472988,"user_tz":-540,"elapsed":30892,"user":{"displayName":"김민제","userId":"08475270451301796366"}},"outputId":"34fe5f24-42e3-49c4-e3f7-9d318b593e8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","29/29 [==============================] - 1s 10ms/step - loss: 31286.7617 - val_loss: 22256.9629\n","Epoch 2/200\n","29/29 [==============================] - 0s 4ms/step - loss: 31122.5293 - val_loss: 22124.3066\n","Epoch 3/200\n","29/29 [==============================] - 0s 3ms/step - loss: 30901.8164 - val_loss: 21948.4805\n","Epoch 4/200\n","29/29 [==============================] - 0s 3ms/step - loss: 30597.1211 - val_loss: 21700.1523\n","Epoch 5/200\n","29/29 [==============================] - 0s 4ms/step - loss: 30146.5996 - val_loss: 21365.7520\n","Epoch 6/200\n","29/29 [==============================] - 0s 3ms/step - loss: 29507.6270 - val_loss: 20900.7832\n","Epoch 7/200\n","29/29 [==============================] - 0s 3ms/step - loss: 28603.6211 - val_loss: 20269.9648\n","Epoch 8/200\n","29/29 [==============================] - 0s 4ms/step - loss: 27408.9727 - val_loss: 19432.9102\n","Epoch 9/200\n","29/29 [==============================] - 0s 3ms/step - loss: 25877.7988 - val_loss: 18390.3848\n","Epoch 10/200\n","29/29 [==============================] - 0s 3ms/step - loss: 23939.3809 - val_loss: 17091.3652\n","Epoch 11/200\n","29/29 [==============================] - 0s 3ms/step - loss: 21633.8906 - val_loss: 15636.3438\n","Epoch 12/200\n","29/29 [==============================] - 0s 3ms/step - loss: 19087.3203 - val_loss: 14045.6230\n","Epoch 13/200\n","29/29 [==============================] - 0s 3ms/step - loss: 16538.4883 - val_loss: 12465.5566\n","Epoch 14/200\n","29/29 [==============================] - 0s 3ms/step - loss: 14097.1211 - val_loss: 10901.6426\n","Epoch 15/200\n","29/29 [==============================] - 0s 3ms/step - loss: 11915.2021 - val_loss: 9543.4385\n","Epoch 16/200\n","29/29 [==============================] - 0s 3ms/step - loss: 10168.5068 - val_loss: 8358.8760\n","Epoch 17/200\n","29/29 [==============================] - 0s 3ms/step - loss: 8851.4395 - val_loss: 7416.7520\n","Epoch 18/200\n","29/29 [==============================] - 0s 3ms/step - loss: 7888.1836 - val_loss: 6668.1553\n","Epoch 19/200\n","29/29 [==============================] - 0s 3ms/step - loss: 7148.0200 - val_loss: 6037.8770\n","Epoch 20/200\n","29/29 [==============================] - 0s 3ms/step - loss: 6598.5791 - val_loss: 5558.9756\n","Epoch 21/200\n","29/29 [==============================] - 0s 3ms/step - loss: 6145.8804 - val_loss: 5150.7012\n","Epoch 22/200\n","29/29 [==============================] - 0s 4ms/step - loss: 5765.0864 - val_loss: 4750.5015\n","Epoch 23/200\n","29/29 [==============================] - 0s 4ms/step - loss: 5433.0962 - val_loss: 4465.2051\n","Epoch 24/200\n","29/29 [==============================] - 0s 3ms/step - loss: 5153.9189 - val_loss: 4212.5591\n","Epoch 25/200\n","29/29 [==============================] - 0s 3ms/step - loss: 4932.9800 - val_loss: 4036.5613\n","Epoch 26/200\n","29/29 [==============================] - 0s 3ms/step - loss: 4752.8389 - val_loss: 3888.1060\n","Epoch 27/200\n","29/29 [==============================] - 0s 3ms/step - loss: 4601.1660 - val_loss: 3753.2622\n","Epoch 28/200\n","29/29 [==============================] - 0s 4ms/step - loss: 4473.3164 - val_loss: 3680.2253\n","Epoch 29/200\n","29/29 [==============================] - 0s 3ms/step - loss: 4380.7642 - val_loss: 3601.4368\n","Epoch 30/200\n","29/29 [==============================] - 0s 3ms/step - loss: 4291.7573 - val_loss: 3536.1802\n","Epoch 31/200\n","29/29 [==============================] - 0s 4ms/step - loss: 4216.0605 - val_loss: 3484.5737\n","Epoch 32/200\n","29/29 [==============================] - 0s 4ms/step - loss: 4148.4575 - val_loss: 3441.7874\n","Epoch 33/200\n","29/29 [==============================] - 0s 3ms/step - loss: 4080.3918 - val_loss: 3397.6238\n","Epoch 34/200\n","29/29 [==============================] - 0s 4ms/step - loss: 4034.9602 - val_loss: 3368.5293\n","Epoch 35/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3979.0396 - val_loss: 3323.6125\n","Epoch 36/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3925.4517 - val_loss: 3289.4348\n","Epoch 37/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3876.5669 - val_loss: 3259.9983\n","Epoch 38/200\n","29/29 [==============================] - 0s 5ms/step - loss: 3835.4331 - val_loss: 3227.7878\n","Epoch 39/200\n","29/29 [==============================] - 0s 5ms/step - loss: 3792.4631 - val_loss: 3214.6384\n","Epoch 40/200\n","29/29 [==============================] - 0s 5ms/step - loss: 3755.2034 - val_loss: 3187.5752\n","Epoch 41/200\n","29/29 [==============================] - 0s 5ms/step - loss: 3718.9124 - val_loss: 3175.8025\n","Epoch 42/200\n","29/29 [==============================] - 0s 5ms/step - loss: 3678.6724 - val_loss: 3144.4841\n","Epoch 43/200\n","29/29 [==============================] - 0s 4ms/step - loss: 3641.0994 - val_loss: 3132.3896\n","Epoch 44/200\n","29/29 [==============================] - 0s 4ms/step - loss: 3608.3467 - val_loss: 3120.4226\n","Epoch 45/200\n","29/29 [==============================] - 0s 4ms/step - loss: 3578.2146 - val_loss: 3095.6975\n","Epoch 46/200\n","29/29 [==============================] - 0s 5ms/step - loss: 3548.1982 - val_loss: 3081.7324\n","Epoch 47/200\n","29/29 [==============================] - 0s 4ms/step - loss: 3518.6284 - val_loss: 3072.3970\n","Epoch 48/200\n","29/29 [==============================] - 0s 5ms/step - loss: 3490.3462 - val_loss: 3053.0657\n","Epoch 49/200\n","29/29 [==============================] - 0s 4ms/step - loss: 3461.0295 - val_loss: 3047.2712\n","Epoch 50/200\n","29/29 [==============================] - 0s 4ms/step - loss: 3437.5420 - val_loss: 3030.1338\n","Epoch 51/200\n","29/29 [==============================] - 0s 4ms/step - loss: 3415.4963 - val_loss: 3022.0930\n","Epoch 52/200\n","29/29 [==============================] - 0s 4ms/step - loss: 3387.2598 - val_loss: 3021.1523\n","Epoch 53/200\n","29/29 [==============================] - 0s 4ms/step - loss: 3364.1704 - val_loss: 2999.2397\n","Epoch 54/200\n","29/29 [==============================] - 0s 4ms/step - loss: 3340.4993 - val_loss: 2980.7573\n","Epoch 55/200\n","29/29 [==============================] - 0s 5ms/step - loss: 3320.9275 - val_loss: 2970.5083\n","Epoch 56/200\n","29/29 [==============================] - 0s 5ms/step - loss: 3298.5120 - val_loss: 2971.0620\n","Epoch 57/200\n","29/29 [==============================] - 0s 4ms/step - loss: 3278.1748 - val_loss: 2958.6401\n","Epoch 58/200\n","29/29 [==============================] - 0s 7ms/step - loss: 3258.2412 - val_loss: 2954.1550\n","Epoch 59/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3238.7097 - val_loss: 2951.5549\n","Epoch 60/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3216.8535 - val_loss: 2945.4697\n","Epoch 61/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3205.7542 - val_loss: 2941.6228\n","Epoch 62/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3181.5640 - val_loss: 2932.6545\n","Epoch 63/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3167.9707 - val_loss: 2938.2571\n","Epoch 64/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3149.3262 - val_loss: 2940.3413\n","Epoch 65/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3138.9109 - val_loss: 2934.4014\n","Epoch 66/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3122.4246 - val_loss: 2933.2725\n","Epoch 67/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3108.7722 - val_loss: 2923.0061\n","Epoch 68/200\n","29/29 [==============================] - 0s 4ms/step - loss: 3090.5190 - val_loss: 2921.1997\n","Epoch 69/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3074.3569 - val_loss: 2914.3982\n","Epoch 70/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3061.8845 - val_loss: 2912.1338\n","Epoch 71/200\n","29/29 [==============================] - 0s 4ms/step - loss: 3059.8376 - val_loss: 2904.2512\n","Epoch 72/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3043.6001 - val_loss: 2906.2358\n","Epoch 73/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3031.8340 - val_loss: 2905.1755\n","Epoch 74/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3022.7312 - val_loss: 2902.1321\n","Epoch 75/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3010.6685 - val_loss: 2907.2576\n","Epoch 76/200\n","29/29 [==============================] - 0s 3ms/step - loss: 3003.5325 - val_loss: 2907.7439\n","Epoch 77/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2990.6431 - val_loss: 2909.2051\n","Epoch 78/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2984.3423 - val_loss: 2909.9189\n","Epoch 79/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2970.1311 - val_loss: 2910.5457\n","Epoch 80/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2966.4612 - val_loss: 2915.9280\n","Epoch 81/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2958.4976 - val_loss: 2913.7700\n","Epoch 82/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2944.5957 - val_loss: 2919.7407\n","Epoch 83/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2939.5735 - val_loss: 2914.8186\n","Epoch 84/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2937.4094 - val_loss: 2922.1021\n","Epoch 85/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2927.6648 - val_loss: 2929.9470\n","Epoch 86/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2920.0728 - val_loss: 2918.3086\n","Epoch 87/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2909.9814 - val_loss: 2923.3865\n","Epoch 88/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2902.3481 - val_loss: 2926.3918\n","Epoch 89/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2895.9607 - val_loss: 2915.3892\n","Epoch 90/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2887.8394 - val_loss: 2917.2437\n","Epoch 91/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2887.1965 - val_loss: 2915.5754\n","Epoch 92/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2879.3679 - val_loss: 2916.0283\n","Epoch 93/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2870.8840 - val_loss: 2913.6641\n","Epoch 94/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2865.9932 - val_loss: 2910.8899\n","Epoch 95/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2859.0647 - val_loss: 2907.3743\n","Epoch 96/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2857.0969 - val_loss: 2905.7695\n","Epoch 97/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2843.8142 - val_loss: 2905.0371\n","Epoch 98/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2856.6736 - val_loss: 2917.7349\n","Epoch 99/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2835.3037 - val_loss: 2904.4746\n","Epoch 100/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2830.7197 - val_loss: 2908.7158\n","Epoch 101/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2825.2009 - val_loss: 2900.6233\n","Epoch 102/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2821.9978 - val_loss: 2905.6399\n","Epoch 103/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2815.6118 - val_loss: 2917.0862\n","Epoch 104/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2809.2537 - val_loss: 2923.7913\n","Epoch 105/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2806.8423 - val_loss: 2916.7114\n","Epoch 106/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2805.3450 - val_loss: 2915.1001\n","Epoch 107/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2806.5068 - val_loss: 2901.1672\n","Epoch 108/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2797.9062 - val_loss: 2899.7080\n","Epoch 109/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2790.7419 - val_loss: 2900.6858\n","Epoch 110/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2795.0090 - val_loss: 2903.7869\n","Epoch 111/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2788.4773 - val_loss: 2900.0176\n","Epoch 112/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2781.9712 - val_loss: 2896.0176\n","Epoch 113/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2781.7048 - val_loss: 2889.4922\n","Epoch 114/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2774.0955 - val_loss: 2894.1306\n","Epoch 115/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2773.2571 - val_loss: 2907.5457\n","Epoch 116/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2772.3921 - val_loss: 2893.9246\n","Epoch 117/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2765.2266 - val_loss: 2895.1021\n","Epoch 118/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2763.2253 - val_loss: 2903.0334\n","Epoch 119/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2760.7468 - val_loss: 2896.9670\n","Epoch 120/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2755.9580 - val_loss: 2892.5024\n","Epoch 121/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2753.6528 - val_loss: 2898.4780\n","Epoch 122/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2747.4417 - val_loss: 2904.7947\n","Epoch 123/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2747.8760 - val_loss: 2896.0186\n","Epoch 124/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2750.6746 - val_loss: 2915.3750\n","Epoch 125/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2744.4949 - val_loss: 2928.5256\n","Epoch 126/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2738.1431 - val_loss: 2910.5200\n","Epoch 127/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2732.2371 - val_loss: 2921.7151\n","Epoch 128/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2733.4939 - val_loss: 2918.6724\n","Epoch 129/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2737.1978 - val_loss: 2927.6562\n","Epoch 130/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2728.3425 - val_loss: 2917.0229\n","Epoch 131/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2720.8035 - val_loss: 2896.0823\n","Epoch 132/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2729.9958 - val_loss: 2895.5486\n","Epoch 133/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2721.0042 - val_loss: 2910.6265\n","Epoch 134/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2720.1670 - val_loss: 2897.4734\n","Epoch 135/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2724.0842 - val_loss: 2900.5859\n","Epoch 136/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2711.0042 - val_loss: 2893.5610\n","Epoch 137/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2720.6384 - val_loss: 2889.0889\n","Epoch 138/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2705.6455 - val_loss: 2895.7932\n","Epoch 139/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2706.0544 - val_loss: 2898.8921\n","Epoch 140/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2703.1636 - val_loss: 2883.1804\n","Epoch 141/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2703.2007 - val_loss: 2901.7551\n","Epoch 142/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2707.2964 - val_loss: 2895.7952\n","Epoch 143/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2697.4624 - val_loss: 2914.2939\n","Epoch 144/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2696.9517 - val_loss: 2902.7786\n","Epoch 145/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2700.1516 - val_loss: 2901.7114\n","Epoch 146/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2699.5596 - val_loss: 2924.1318\n","Epoch 147/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2699.4858 - val_loss: 2897.5212\n","Epoch 148/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2688.2891 - val_loss: 2910.9380\n","Epoch 149/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2689.5488 - val_loss: 2895.4504\n","Epoch 150/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2687.1699 - val_loss: 2902.0151\n","Epoch 151/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2688.9326 - val_loss: 2904.2378\n","Epoch 152/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2687.6572 - val_loss: 2894.8335\n","Epoch 153/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2680.3896 - val_loss: 2914.4331\n","Epoch 154/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2685.0557 - val_loss: 2917.6365\n","Epoch 155/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2683.5938 - val_loss: 2914.6101\n","Epoch 156/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2690.0562 - val_loss: 2896.5881\n","Epoch 157/200\n","29/29 [==============================] - 0s 5ms/step - loss: 2676.5015 - val_loss: 2902.2917\n","Epoch 158/200\n","29/29 [==============================] - 0s 5ms/step - loss: 2673.5437 - val_loss: 2919.9910\n","Epoch 159/200\n","29/29 [==============================] - 0s 5ms/step - loss: 2672.5959 - val_loss: 2920.0986\n","Epoch 160/200\n","29/29 [==============================] - 0s 5ms/step - loss: 2670.4958 - val_loss: 2915.6750\n","Epoch 161/200\n","29/29 [==============================] - 0s 5ms/step - loss: 2668.9929 - val_loss: 2929.4607\n","Epoch 162/200\n","29/29 [==============================] - 0s 6ms/step - loss: 2673.4961 - val_loss: 2917.6841\n","Epoch 163/200\n","29/29 [==============================] - 0s 5ms/step - loss: 2674.6599 - val_loss: 2937.9788\n","Epoch 164/200\n","29/29 [==============================] - 0s 5ms/step - loss: 2670.0212 - val_loss: 2926.9001\n","Epoch 165/200\n","29/29 [==============================] - 0s 5ms/step - loss: 2665.9155 - val_loss: 2895.1377\n","Epoch 166/200\n","29/29 [==============================] - 0s 8ms/step - loss: 2667.0786 - val_loss: 2877.3364\n","Epoch 167/200\n","29/29 [==============================] - 0s 12ms/step - loss: 2663.7930 - val_loss: 2883.7285\n","Epoch 168/200\n","29/29 [==============================] - 0s 12ms/step - loss: 2659.8643 - val_loss: 2887.0366\n","Epoch 169/200\n","29/29 [==============================] - 0s 12ms/step - loss: 2656.7151 - val_loss: 2897.8179\n","Epoch 170/200\n","29/29 [==============================] - 0s 11ms/step - loss: 2661.2285 - val_loss: 2901.1523\n","Epoch 171/200\n","29/29 [==============================] - 0s 10ms/step - loss: 2654.4810 - val_loss: 2907.8286\n","Epoch 172/200\n","29/29 [==============================] - 0s 12ms/step - loss: 2651.0481 - val_loss: 2886.8333\n","Epoch 173/200\n","29/29 [==============================] - 0s 7ms/step - loss: 2651.7043 - val_loss: 2899.0664\n","Epoch 174/200\n","29/29 [==============================] - 0s 5ms/step - loss: 2648.9602 - val_loss: 2912.2334\n","Epoch 175/200\n","29/29 [==============================] - 0s 6ms/step - loss: 2646.3203 - val_loss: 2907.5439\n","Epoch 176/200\n","29/29 [==============================] - 0s 7ms/step - loss: 2648.2283 - val_loss: 2925.5190\n","Epoch 177/200\n","29/29 [==============================] - 0s 5ms/step - loss: 2648.0547 - val_loss: 2914.4658\n","Epoch 178/200\n","29/29 [==============================] - 0s 7ms/step - loss: 2642.2942 - val_loss: 2911.8281\n","Epoch 179/200\n","29/29 [==============================] - 0s 11ms/step - loss: 2644.8550 - val_loss: 2921.1597\n","Epoch 180/200\n","29/29 [==============================] - 0s 13ms/step - loss: 2639.9675 - val_loss: 2926.1045\n","Epoch 181/200\n","29/29 [==============================] - 0s 7ms/step - loss: 2648.2000 - val_loss: 2929.5186\n","Epoch 182/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2637.6396 - val_loss: 2911.8623\n","Epoch 183/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2633.2527 - val_loss: 2902.0964\n","Epoch 184/200\n","29/29 [==============================] - 0s 6ms/step - loss: 2637.1763 - val_loss: 2903.3696\n","Epoch 185/200\n","29/29 [==============================] - 0s 6ms/step - loss: 2629.7224 - val_loss: 2913.1831\n","Epoch 186/200\n","29/29 [==============================] - 0s 5ms/step - loss: 2632.4592 - val_loss: 2908.6003\n","Epoch 187/200\n","29/29 [==============================] - 0s 5ms/step - loss: 2636.5920 - val_loss: 2905.6470\n","Epoch 188/200\n","29/29 [==============================] - 0s 5ms/step - loss: 2627.8857 - val_loss: 2928.8533\n","Epoch 189/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2632.3684 - val_loss: 2945.8137\n","Epoch 190/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2629.6145 - val_loss: 2921.0881\n","Epoch 191/200\n","29/29 [==============================] - 0s 4ms/step - loss: 2625.7234 - val_loss: 2930.2578\n","Epoch 192/200\n","29/29 [==============================] - 0s 8ms/step - loss: 2627.0737 - val_loss: 2928.4180\n","Epoch 193/200\n","29/29 [==============================] - 0s 7ms/step - loss: 2624.0105 - val_loss: 2929.9822\n","Epoch 194/200\n","29/29 [==============================] - 0s 8ms/step - loss: 2625.0525 - val_loss: 2933.5486\n","Epoch 195/200\n","29/29 [==============================] - 0s 7ms/step - loss: 2617.3684 - val_loss: 2928.0505\n","Epoch 196/200\n","29/29 [==============================] - 0s 6ms/step - loss: 2621.2000 - val_loss: 2929.4812\n","Epoch 197/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2617.3469 - val_loss: 2934.4849\n","Epoch 198/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2616.7283 - val_loss: 2941.8716\n","Epoch 199/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2616.7698 - val_loss: 2932.3225\n","Epoch 200/200\n","29/29 [==============================] - 0s 3ms/step - loss: 2617.5149 - val_loss: 2942.3596\n","3/3 [==============================] - 0s 3ms/step\n","R^2 score: 0.4775978015430824\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_JUB58Z4Yi3c"},"execution_count":null,"outputs":[]}]}